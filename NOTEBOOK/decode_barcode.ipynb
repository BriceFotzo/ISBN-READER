{
 "cells": [
  {
   "source": [
    "# Ce notebook s'articule en 5 grandes parties :\n",
    "\n",
    "\n",
    "###  Importation des packages\n",
    "###  Définition des fonctions\n",
    "###  Traitement des images\n",
    "###  Choix et construction des modèles\n",
    "###  Implémentation et comparaison des modèles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Importation des packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\a020681\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from imutils.perspective import four_point_transform\n",
    "from imutils import contours\n",
    "import imutils\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "import os,sys\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "source": [
    "## Définition des fonctions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_canny(image, sigma=0.33):\n",
    "    # compute the median of the single channel pixel intensities\n",
    "    v = np.median(image)\n",
    "    # apply automatic Canny edge detection using the computed median\n",
    "    lower = int(max(0, (1.0 - sigma) * v))\n",
    "    upper = int(min(255, (1.0 + sigma) * v))\n",
    "    edged = cv2.Canny(image, lower, upper)\n",
    "    # return the edged image\n",
    "    return edged\n",
    "def sequence(images,cx,cy):\n",
    "    \n",
    "    #gray=cv2.fastNlMeansDenoisingColored(images,None,1,8,3,3)\n",
    "    #gray=cv2.fastNlMeansDenoisingColored(gray,None,3,1,3,1)\n",
    "\n",
    "    gray = cv2.cvtColor(images, cv2.COLOR_BGR2GRAY)\n",
    "    gray=cv2.fastNlMeansDenoising(gray,None,10,7,21)\n",
    "    threshes =  cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "            cv2.THRESH_BINARY_INV,11,3)\n",
    "\n",
    "    edgeds = auto_canny(gray)\n",
    "    \n",
    "    kernels = cv2.getStructuringElement(cv2.MORPH_RECT, (cx,cy))\n",
    "    closeds = cv2.morphologyEx(edgeds, cv2.MORPH_CLOSE, kernels)\n",
    "    #closeds = cv2.erode(closeds, None, iterations = 1)\n",
    "    closeds = cv2.dilate(closeds, None, iterations = 3)\n",
    "    cnts = cv2.findContours(closeds.copy(), cv2.RETR_EXTERNAL,\n",
    "    cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    cnts_=[cnt for cnt in cnts if cv2.boundingRect(cnt)[2]/images.shape[1]<=0.3 ]\n",
    "    c = sorted(cnts_, key = cv2.contourArea, reverse = True)[0]\n",
    "    # compute the rotated bounding box of the largest contour\n",
    "    rect = cv2.minAreaRect(c)\n",
    "    box = cv2.cv.BoxPoints(rect) if imutils.is_cv2() else cv2.boxPoints(rect)\n",
    "    box = np.int0(box)\n",
    "    all_areas=[cv2.contourArea(rect) for rect in cnts]\n",
    "    total_area=np.sum(all_areas)\n",
    "    return cnts,all_areas,closeds,threshes,images,box\n",
    "def process(images,cx,cy):\n",
    "    \n",
    "    #gray=cv2.fastNlMeansDenoisingColored(images,None,1,8,3,3)\n",
    "    #gray=cv2.fastNlMeansDenoisingColored(gray,None,3,1,3,1)\n",
    "\n",
    "    gray = cv2.cvtColor(images, cv2.COLOR_BGR2GRAY)\n",
    "    gray=cv2.fastNlMeansDenoising(gray,None,10,7,21)\n",
    "    threshes =  cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "            cv2.THRESH_BINARY_INV,11,3)\n",
    "\n",
    "    edgeds = auto_canny(gray)\n",
    "    \n",
    "    kernels = cv2.getStructuringElement(cv2.MORPH_RECT, (cx,cy))\n",
    "    closeds = cv2.morphologyEx(edgeds, cv2.MORPH_CLOSE, kernels)\n",
    "    closeds = cv2.erode(closeds, None, iterations = 1)\n",
    "    #closeds = cv2.dilate(closeds, None, iterations = 3)\n",
    "    cnts = cv2.findContours(closeds.copy(), cv2.RETR_EXTERNAL,\n",
    "    cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    cnts_=[cnt for cnt in cnts if cv2.boundingRect(cnt)[2]/images.shape[1]<=0.3 ]\n",
    "    all_areas=[cv2.contourArea(rect) for rect in cnts]\n",
    "    total_area=np.sum(all_areas)\n",
    "    return cnts,all_areas,closeds,threshes,images\n",
    "def find_digits(images,cx,cy):\n",
    "    \n",
    "    gray = cv2.cvtColor(top, cv2.COLOR_BGR2GRAY)\n",
    "    #gray=cv2.fastNlMeansDenoising(gray,None,10,7,7)\n",
    "    threshes =  cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "            cv2.THRESH_BINARY_INV,255,17)\n",
    "\n",
    "    edgeds = auto_canny(threshes)\n",
    "\n",
    "    kernels = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "    clo = cv2.morphologyEx(threshes, cv2.MORPH_CLOSE, kernels)\n",
    "    cnts = cv2.findContours(clo.copy(), cv2.RETR_EXTERNAL,\n",
    "    cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    for cnt in cnts:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        imc=top.copy()\n",
    "        #if (y > 2*image.shape[0]/3) | (y < image.shape[0]/3) & (y+h < image.shape[0]/3):\n",
    "        #bound the images\n",
    "        cv2.rectangle(top,(x,y),(x+w,y+h),(0,0,255),1)\n",
    "    return cnts,all_areas,closeds,threshes,images,box\n",
    "\n",
    "\n",
    "def draw_countours(image,original,out,cx,cy,digit,Rx,Ry):\n",
    "    if digit:\n",
    "        sequantial=find_digits(image,cx,cy)\n",
    "        \n",
    "    else:\n",
    "        sequantial=sequence(image,cx,cy)\n",
    "    contours=sequantial[0]\n",
    "    all_areas=sequantial[1]\n",
    "    image=sequantial[4]\n",
    "    box=sequantial[5]\n",
    "    x_,y_,w_,h_=cv2.boundingRect(box)\n",
    "    x,y,w,h=int(x_/Rx),int(y_/Ry),int(w_/Rx),int(h_/Ry)\n",
    "    height=[]\n",
    "    width=[]\n",
    "    total_area=np.sum(all_areas)\n",
    "    for cnt in contours:\n",
    "        #x,y,w,h = cv2.boundingRect(cnt)\n",
    "        imc=image.copy()\n",
    "        #if (y > 2*image.shape[0]/3) | (y < image.shape[0]/3) & (y+h < image.shape[0]/3):\n",
    "        #bound the images\n",
    "        #cv2.rectangle(image,(x,y),(x+w,y+h),(0,0,255),1)\n",
    "        cv2.drawContours(imc, [box], -1, (0, 255, 0), 3)\n",
    "        cv2.imwrite(\"result/\"+out+\".jpg\",original[y-3:y+h+5,x-3:x+w+3])\n",
    "        return imc\n",
    "def resize(image,how,width,height):\n",
    "    if how=='up':\n",
    "        image=cv2.resize(image,(int(image.shape[1]*width),int(image.shape[0]*height)))\n",
    "    elif how=='down':\n",
    "        image=cv2.resize(image,(int(image.shape[1]/width),int(image.shape[0]/height)))\n",
    "    else:\n",
    "        image=cv2.resize(image,(width,height))\n",
    "    return image,width,height\n",
    "        \n",
    "import numpy as np\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "def get_pos(char):\n",
    "    return int(char.split('.')[0])"
   ]
  },
  {
   "source": [
    "## Traitement des images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_to_split=cv2.imread(\"output/r0.jpg\")\n",
    "im_r_to_split=resize(im_to_split,'down',8,8)[0]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#im_top=im_r_to_split[:3*int(im_r_to_split.shape[0]/10),:]\n",
    "im_bottom_ori=im_r_to_split[7*int(im_r_to_split.shape[0]/10):,:]\n",
    "#im_top=cv2.resize(im_top,(im_top.shape[1]*3,im_top.shape[0]*5))\n",
    "#im_bottom=cv2.resize(im_bottom,(im_bottom.shape[1]*3,im_bottom.shape[0]*5))\n",
    "\n",
    "im_bottom_r,Rx,Ry=resize(im_bottom_ori.copy(),'up',3,5)\n",
    "\n",
    "\n",
    "#im_top=draw_countours(im_top,'top_',50,20,False)\n",
    "im_bottom_r=draw_countours(im_bottom_r,im_bottom_ori,'code_0',50,20,False,Rx,Ry)\n",
    "#s_top=sequence(im_top,50,20)\n",
    "s_bottom=sequence(im_bottom_r,50,20)\n",
    "#cl_top=s_top[2]\n",
    "cl_bottom=s_bottom[2]\n",
    "#cv2.imshow('Top',im_top)\n",
    "cv2.imshow('Bottom',im_bottom_r)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a020681\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:43: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "C:\\Users\\a020681\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "C:\\Users\\a020681\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "top=cv2.imread(\"result/code_1.jpg\")\n",
    "top=cv2.resize(top,(top.shape[1]*4,top.shape[0]*2))\n",
    "gray = cv2.cvtColor(top, cv2.COLOR_BGR2GRAY)\n",
    "#gray=cv2.fastNlMeansDenoising(gray,None,10,7,7)\n",
    "threshes =  cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "        cv2.THRESH_BINARY_INV,15,5)\n",
    "\n",
    "edgeds = auto_canny(threshes)\n",
    "\n",
    "kernels = cv2.getStructuringElement(cv2.MORPH_RECT, (3,2))\n",
    "clo = cv2.morphologyEx(threshes, cv2.MORPH_CLOSE, kernels,iterations=1)\n",
    "#clo = cv2.erode(clo, None, iterations = 1)\n",
    "cnts = cv2.findContours(clo.copy(), cv2.RETR_EXTERNAL,\n",
    "cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = imutils.grab_contours(cnts)\n",
    "all_areas=[cv2.contourArea(rect) for rect in cnts]\n",
    "\n",
    "if len(all_areas)<15:\n",
    "    gray=resize(gray,'up',2,3)[0]\n",
    "    top_,Rx,Ry=resize(top.copy(),'up',2,3)\n",
    "\n",
    "\n",
    "    threshes =  cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "        cv2.THRESH_BINARY_INV,127,40)\n",
    "\n",
    "    edgeds = auto_canny(threshes)\n",
    "    threshes= cv2.dilate(threshes, None, iterations = 2)\n",
    "    kernels = cv2.getStructuringElement(cv2.MORPH_RECT, (8,11))\n",
    "    clo = cv2.morphologyEx(threshes, cv2.MORPH_CLOSE, kernels,iterations=1)\n",
    "    #c=sorted(cnts, key = cv2.contourArea, reverse = True)[0]\n",
    "    #rect = cv2.minAreaRect(c)\n",
    "    #box = cv2.cv.BoxPoints(rect) if imutils.is_cv2() else cv2.boxPoints(rect)\n",
    "    #box = np.int0(box)\n",
    "\n",
    "    cnts = cv2.findContours(clo.copy(), cv2.RETR_EXTERNAL,\n",
    "    cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    all_areas=[cv2.contourArea(rect) for rect in cnts]\n",
    "else :\n",
    "    top_,Rx,Ry=top.copy(),1,1\n",
    "\n",
    "height=pd.Series([])\n",
    "width=pd.Series([])\n",
    "areas_digits=pd.Series([])\n",
    "for i,cnt in enumerate(cnts[:13]):\n",
    "    x,y,w,h = cv2.boundingRect(cnt)\n",
    "    x_,y_,w_,h_=int(x/Rx),int(y/Ry),int(w/Rx),int(h/Ry)\n",
    "    height[i]=y+h\n",
    "    width[i]=x+w\n",
    "    imc=top_.copy()\n",
    "    #if (y > 2*image.shape[0]/3) | (y < image.shape[0]/3) & (y+h < image.shape[0]/3):\n",
    "    #bound the images\n",
    "    if (w/h <=2) & (w/h>=0.1) & (h/imc.shape[0]<0.3) :\n",
    "        areas_digits[i]=cv2.contourArea(cnt)\n",
    "        cv2.drawContours(top_, [cnt], -1, (0, 255, 0), 1)\n",
    "        cv2.rectangle(top_,(x,y),(x+w,y+h),(0,0,255),1)\n",
    "        cv2.imwrite(\"digits/\"+str(i)+\".jpg\",top[y_-3:y_+h_+3,x_-3:x_+w_+3])\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "cv2.imshow('Top',top)\n",
    "cv2.imshow('clos',clo)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "source": [
    "## Choix et construction des modèles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=np.loadtxt(\"../OpenCV_3_KNN_Character_Recognition_Python-master/flattened_images.txt\", np.float32)\n",
    "classes=np.loadtxt(\"../OpenCV_3_KNN_Character_Recognition_Python-master/classifications.txt\", np.float32)\n",
    "labels=classes.reshape(class_np.size, 1)\n",
    "classes=[chr(c) for c in classes]\n",
    "targets=pd.get_dummies(pd.DataFrame(classes)).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif = OneVsRestClassifier(estimator=SVC(random_state=0))\n",
    "clf=svm.SVC(gamma=0.0001,C=0.001)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "kNearest = cv2.ml.KNearest_create() \n",
    "classif.fit(images[66:],classes[66:])\n",
    "clf.fit(images[66:],classes[66:])\n",
    "knn.fit(images[66:],classes[66:])\n",
    "kNearest.train(images, cv2.ml.ROW_SAMPLE,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 \t| Loss = 364.92197 \t| Accuracy = 0.22727273\n",
      "Iteration 100 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 200 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 300 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 400 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 500 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 600 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 700 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 800 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 900 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 1000 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 1100 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 1200 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 1300 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 1400 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 1500 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 1600 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 1700 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 1800 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 1900 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 2000 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 2100 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 2200 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 2300 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 2400 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 2500 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 2600 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 2700 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 2800 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 2900 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 3000 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 3100 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 3200 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 3300 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 3400 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 3500 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 3600 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 3700 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 3800 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 3900 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 4000 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 4100 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 4200 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 4300 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 4400 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 4500 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 4600 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 4700 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 4800 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 4900 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 5000 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 5100 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 5200 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 5300 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 5400 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 5500 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 5600 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 5700 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 5800 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 5900 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 6000 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 6100 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 6200 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 6300 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 6400 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 6500 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 6600 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 6700 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 6800 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 6900 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 7000 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 7100 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 7200 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 7300 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 7400 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 7500 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 7600 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 7700 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 7800 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 7900 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 8000 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 8100 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 8200 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 8300 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 8400 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 8500 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 8600 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 8700 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 8800 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 8900 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 9000 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 9100 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 9200 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 9300 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 9400 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 9500 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 9600 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 9700 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 9800 \t| Loss = 0.0 \t| Accuracy = 1.0\n",
      "Iteration 9900 \t| Loss = 0.0 \t| Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "n_input = 600  # input layer (28x28 pixels)\n",
    "n_hidden1 =  480  # 1st hidden layer\n",
    "n_hidden2 = 240  # 2nd hidden layer\n",
    "n_hidden3 = 120   # 3rd hidden layer\n",
    "n_hidden4 = 60  # 3rd hidden layer\n",
    "#n_hidden5 = 60  # 3rd hidden layer\n",
    "n_output = 10  # output layer (0-9 digits)\n",
    "#hyper parameters\n",
    "learning_rate = 1e-4\n",
    "n_iterations =10000\n",
    "batch_size = 128\n",
    "dropout = 0.5\n",
    "...\n",
    "n_train = len(images[66:])  # 55,000\n",
    "#n_validation = mnist.validation.num_examples  # 5000\n",
    "#n_test = mnist.test.num_examples  # 10,000\n",
    "\n",
    "# init X and y\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_output])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "weights = {\n",
    "    'w1': tf.Variable(tf.truncated_normal([n_input, n_hidden1], stddev=0.1)),\n",
    "    'w2': tf.Variable(tf.truncated_normal([n_hidden1, n_hidden2], stddev=0.1)),\n",
    "    'w3': tf.Variable(tf.truncated_normal([n_hidden2, n_hidden3], stddev=0.1)),\n",
    "    'w4': tf.Variable(tf.truncated_normal([n_hidden3, n_hidden4], stddev=0.1)),\n",
    "    #'w5': tf.Variable(tf.truncated_normal([n_hidden4, n_hidden5], stddev=0.1)),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden4, n_output], stddev=0.1)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.constant(0.1, shape=[n_hidden1])),\n",
    "    'b2': tf.Variable(tf.constant(0.1, shape=[n_hidden2])),\n",
    "    'b3': tf.Variable(tf.constant(0.1, shape=[n_hidden3])),\n",
    "    'b4': tf.Variable(tf.constant(0.1, shape=[n_hidden4])),\n",
    "  #  'b5': tf.Variable(tf.constant(0.1, shape=[n_hidden5])),\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_output]))\n",
    "}\n",
    "\n",
    "\n",
    "layer_1 = tf.add(tf.matmul(X, weights['w1']), biases['b1'])\n",
    "layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "layer_3 = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
    "layer_4 = tf.add(tf.matmul(layer_3, weights['w4']), biases['b4'])\n",
    "#layer_5 = tf.add(tf.matmul(layer_4, weights['w5']), biases['b5'])\n",
    "layer_drop = tf.nn.dropout(layer_4, keep_prob)\n",
    "output_layer = tf.matmul(layer_4, weights['out']) + biases['out']\n",
    "\n",
    "...\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=Y, logits=output_layer\n",
    "        ))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "...\n",
    "correct_pred = tf.equal(tf.argmax(output_layer, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "...\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# train on mini batches\n",
    "for i in range(n_iterations):\n",
    "    batch_x, batch_y = next_batch(22,images[66:],targets[66:])\n",
    "    sess.run(train_step, feed_dict={\n",
    "        X: batch_x, Y: batch_y, keep_prob: dropout\n",
    "        })\n",
    "\n",
    "    # print loss and accuracy (per minibatch)\n",
    "    if i % 100 == 0:\n",
    "        minibatch_loss, minibatch_accuracy = sess.run(\n",
    "            [cross_entropy, accuracy],\n",
    "            feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0}\n",
    "            )\n",
    "        print(\n",
    "            \"Iteration\",\n",
    "            str(i),\n",
    "            \"\\t| Loss =\",\n",
    "            str(minibatch_loss),\n",
    "            \"\\t| Accuracy =\",\n",
    "            str(minibatch_accuracy)\n",
    "            )"
   ]
  },
  {
   "source": [
    "## Implémentation et comparaison des modèles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsRest Sklearn : 8978229002882\n",
      "Neural Network Tensorflow : 1978919881691\n",
      "KNN Sklearn: 8978229002882\n",
      "KNN OpenCV: 8978229002882\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 13 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit_files=os.listdir(\"digits\")\n",
    "digit_files=sorted(digit_files, key=get_pos,reverse=True)\n",
    "nn_tf_list,knn_list,knn_cv_list,ovr_list=[],[],[],[]\n",
    "for i,digit in enumerate(digit_files,1):\n",
    "    im=cv2.imread(\"digits/\"+digit)\n",
    "    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    im_r=resize(gray,'t',20,30)[0]\n",
    "\n",
    "    # get grayscale image\n",
    "    imgBlurred = cv2.GaussianBlur(im_r, (9,9), 0)                        # blur\n",
    "\n",
    "                                                        # filter image from grayscale to black and white\n",
    "    imgThresh = cv2.adaptiveThreshold(imgBlurred,                           # input image\n",
    "                                      255,                                  # make pixels that pass the threshold full white\n",
    "                                      cv2.ADAPTIVE_THRESH_GAUSSIAN_C,       # use gaussian rather than mean, seems to give better results\n",
    "                                      cv2.THRESH_BINARY_INV,                # invert so foreground will be white, background will be black\n",
    "                                      41,                                   # size of a pixel neighborhood used to calculate threshold value\n",
    "                                      19)\n",
    "    imgThreshCopy = imgThresh.copy()   \n",
    "    npaContours, npaHierarchy = cv2.findContours(imgThreshCopy,        # input image, make sure to use a copy since the function will modify this image in the course of finding contours\n",
    "                                                     cv2.RETR_EXTERNAL,                 # retrieve the outermost contours only\n",
    "                                                     cv2.CHAIN_APPROX_SIMPLE) \n",
    "\n",
    "    for npaContour in npaContours:                          # for each contour      # if contour is big enough to consider\n",
    "        [intX, intY, intW, intH] = cv2.boundingRect(npaContour)         # get and break out bounding rect\n",
    "\n",
    "                                            # draw rectangle around each contour as we ask user for input\n",
    "        cv2.rectangle(im_r,           # draw rectangle on original training image\n",
    "                      (intX, intY),                 # upper left corner\n",
    "                      (intX+intW,intY+intH),        # lower right corner\n",
    "                      (0, 0, 255),                  # red\n",
    "                      2)                            # thickness\n",
    "\n",
    "        imgROI = imgThresh[intY:intY+intH, intX:intX+intW]                                  # crop char out of threshold image\n",
    "        #imgROIResized = cv2.resize(imgROI, (20, 30))     # resize image, this will be more consistent for recognition and storage\n",
    "\n",
    "        #cv2.imshow(\"imgROI\", imgROI)                    # show cropped out char for reference\n",
    "        #cv2.imshow(\"imgROIResized\", imgROIResized)\n",
    "        #cv2.waitKey(0)\n",
    "    im_r_to=resize(imgROI,'t',20,30)[0]\n",
    "    im_r_to_test=im_r_to.reshape((1,30*20))\n",
    "    im_non_processed=imgThresh.reshape((1,30*20))\n",
    "    \n",
    "    plt.subplot(1, 13,i)\n",
    "    plt.imshow(im_r_to,cmap='Greys')\n",
    "    plt.axis('off')\n",
    "    retval, npaResults, neigh_resp, dists = kNearest.findNearest(np.float32(im_r_to_test), k = 1)     # call KNN function find_nearest\n",
    "\n",
    "    strCurrentChar = str(chr(int(npaResults[0][0]))) \n",
    "    prediction = sess.run(tf.argmax(output_layer, 1), feed_dict={X: [im_r_to_test[0]]})\n",
    "    np.squeeze(prediction)\n",
    "\n",
    "    nn_tf_list.append(str(np.squeeze(prediction)))\n",
    "\n",
    "    knn_cv_list.append(strCurrentChar)\n",
    "    ovr_list.append(classif.predict(im_r_to_test)[0])\n",
    "#    svc_list.append(clf.predict(im_r_to_test)[0])\n",
    "    knn_list.append(knn.predict(im_r_to_test)[0])\n",
    "print('OneVsRest Sklearn : {}'.format(''.join(ovr_list)))\n",
    "print('Neural Network Tensorflow : {}'.format(''.join(nn_tf_list)))\n",
    "print('KNN Sklearn: {}'.format(''.join(knn_list)))\n",
    "print('KNN OpenCV: {}'.format(''.join(knn_cv_list)))\n",
    "    \n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}